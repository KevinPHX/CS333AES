{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the syntactic and lexico-syntactic features for argument component identification from the output files in the *src/main/resources folder* within the **preprocessing** Java project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict \n",
    "\n",
    "def get_lca(syn_file): \n",
    "    ''' \n",
    "    Note that c_token means the constituent type of the current token.\n",
    "    Likewise for the preceding and following tokens, i.e., c_preceding and c_following. \n",
    "    '''\n",
    "    info = defaultdict(list)\n",
    "    with open(syn_file,\"r\") as f: \n",
    "        for line in f.readlines(): \n",
    "            line = line.split(\"\\t\")\n",
    "            sentIdx = line[0]\n",
    "            label = line[2]\n",
    "            position = label.split(\"-\")[1]\n",
    "            if sentIdx not in info: \n",
    "                info[sentIdx] = []\n",
    "            token_info = {\n",
    "                \"token\": line[1], \"position\": int(position), \"c_token\": line[3],\n",
    "                \"preceding\": line[4], \"c_preceding\": line[5], \"lcaPath_preceding\": line[6],\n",
    "                \"following\": line[7], \"c_following\": line[8], \"lcaPath_following\": line[9].strip(\"\\n\")\n",
    "            }\n",
    "            info[sentIdx].append(token_info)\n",
    "            break\n",
    "    print(info)\n",
    "\n",
    "def get_lex(lex_file): \n",
    "    ''' \n",
    "    Info for sentences are divided by newlines. \n",
    "    Formatting of info for each sentence: \n",
    "    1. For each token, <token_label>\\t<constituent type of uppermost node>\n",
    "    2. List with each entry being <node_label>\\t<node_index>\n",
    "    3. HashMap mapping <node_index>=<lexical_head>\n",
    "    4. There may be intermediary lines that start with a constituent type. \n",
    "        These are the node labels and types for the children of the uppermost node for each token. \n",
    "    '''\n",
    "    token_info = defaultdict(dict)\n",
    "    node_info = {}\n",
    "    sentIdx = 0\n",
    "    node_indices = {}\n",
    "    lexical_heads = defaultdict(list)\n",
    "    with open(lex_file,\"r\") as f: \n",
    "        for line in f.readlines(): \n",
    "            if line == \"\\n\": # reached a new sentence \n",
    "                node_info[sentIdx] = lexical_heads\n",
    "                print(token_info)\n",
    "                print(node_info)\n",
    "                sentIdx += 1 \n",
    "                node_indices = {}\n",
    "                lexical_heads = {}\n",
    "                if sentIdx == 1: break\n",
    "                continue\n",
    "            if line[0] == \"[\":\n",
    "                # reached info of type 2 \n",
    "                line = line.replace(\"]\",\"\").replace(\"[\",\"\").split(\", \")\n",
    "                for entry in line: \n",
    "                    entry = entry.strip(\"\\n\").split(\"\\t\")\n",
    "                    node_indices[entry[1]] = entry[0]\n",
    "            elif line[0] == \"{\": \n",
    "                # reached info of type 3 \n",
    "                line = line.replace(\"{\",\"\").replace(\"}\",\"\").split(\", \")\n",
    "                for entry in line: \n",
    "                    entry = entry.split(\"=\")\n",
    "                    lexical_heads[entry[0]].append({\"node\": node_indices[entry[0]], \n",
    "                                               \"head\": entry[1] })\n",
    "            else: \n",
    "                line = line.strip(\"\\n\").split(\"\\t\")\n",
    "                token = line[0].split(\"-\")[0]\n",
    "                position = int(line[0].split(\"-\")[1])\n",
    "                if len(line) == 2: \n",
    "                    uppermost = line[1]\n",
    "                    token_dict = { \"token\": token, \"uppermost\": uppermost,\n",
    "                                  \"intermediaries\": []}\n",
    "                    token_info[sentIdx][position] = token_dict\n",
    "                else: # intermediary info \n",
    "                    child = line[1]\n",
    "                    head = line[2]\n",
    "                    token_info[sentIdx][position][\"intermediaries\"].append((child,head))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'0': [{'token': 'It', 'c_token': 'PRP', 'preceding': '', 'c_preceding': '', 'lcaPath_preceding': '-1.000000', 'following': 'is', 'c_following': 'VBZ', 'lcaPath_following': '0.250000'}]})\n",
      "defaultdict(<class 'dict'>, {0: {1: {'token': 'It', 'uppermost': 'NP', 'intermediaries': [('PRP', 'It-1')]}, 2: {'token': 'is', 'uppermost': 'VBZ', 'intermediaries': []}, 3: {'token': 'always', 'uppermost': 'ADVP', 'intermediaries': [('RB', 'always-3')]}, 4: {'token': 'said', 'uppermost': 'ROOT', 'intermediaries': [('S', 'said-4')]}, 5: {'token': 'that', 'uppermost': 'IN', 'intermediaries': []}, 6: {'token': 'competition', 'uppermost': 'NP', 'intermediaries': [('NN', 'competition-6')]}, 7: {'token': 'can', 'uppermost': 'MD', 'intermediaries': []}, 8: {'token': 'effectively', 'uppermost': 'ADVP', 'intermediaries': [('RB', 'effectively-8')]}, 9: {'token': 'promote', 'uppermost': 'SBAR', 'intermediaries': [('IN', 'that-5'), ('S', 'promote-9')]}, 10: {'token': 'the', 'uppermost': 'DT', 'intermediaries': []}, 11: {'token': 'development', 'uppermost': 'NP', 'intermediaries': [('NP', 'development-11'), ('PP', 'economy-13')]}, 12: {'token': 'of', 'uppermost': 'IN', 'intermediaries': []}, 13: {'token': 'economy', 'uppermost': 'PP', 'intermediaries': [('IN', 'of-12'), ('NP', 'economy-13')]}, 14: {'token': '.', 'uppermost': '.', 'intermediaries': []}}})\n",
      "{0: defaultdict(<class 'list'>, {'1': [{'node': 'ROOT', 'head': 'said-4'}], '2': [{'node': 'S', 'head': 'said-4'}], '3': [{'node': 'NP', 'head': 'It-1'}], '4': [{'node': 'PRP', 'head': 'It-1'}], '5': [{'node': 'It-1', 'head': 'It-1'}], '6': [{'node': 'VP', 'head': 'said-4'}], '7': [{'node': 'VBZ', 'head': 'is-2'}], '8': [{'node': 'is-2', 'head': 'is-2'}], '9': [{'node': 'ADVP', 'head': 'always-3'}], '10': [{'node': 'RB', 'head': 'always-3'}], '11': [{'node': 'always-3', 'head': 'always-3'}], '12': [{'node': 'VP', 'head': 'said-4'}], '13': [{'node': 'VBN', 'head': 'said-4'}], '14': [{'node': 'said-4', 'head': 'said-4'}], '15': [{'node': 'SBAR', 'head': 'promote-9'}], '16': [{'node': 'IN', 'head': 'that-5'}], '17': [{'node': 'that-5', 'head': 'that-5'}], '18': [{'node': 'S', 'head': 'promote-9'}], '19': [{'node': 'NP', 'head': 'competition-6'}], '20': [{'node': 'NN', 'head': 'competition-6'}], '21': [{'node': 'competition-6', 'head': 'competition-6'}], '22': [{'node': 'VP', 'head': 'promote-9'}], '23': [{'node': 'MD', 'head': 'can-7'}], '24': [{'node': 'can-7', 'head': 'can-7'}], '25': [{'node': 'ADVP', 'head': 'effectively-8'}], '26': [{'node': 'RB', 'head': 'effectively-8'}], '27': [{'node': 'effectively-8', 'head': 'effectively-8'}], '28': [{'node': 'VP', 'head': 'promote-9'}], '29': [{'node': 'VB', 'head': 'promote-9'}], '30': [{'node': 'promote-9', 'head': 'promote-9'}], '31': [{'node': 'NP', 'head': 'development-11'}], '32': [{'node': 'NP', 'head': 'development-11'}], '33': [{'node': 'DT', 'head': 'the-10'}], '34': [{'node': 'the-10', 'head': 'the-10'}], '35': [{'node': 'NN', 'head': 'development-11'}], '36': [{'node': 'development-11', 'head': 'development-11'}], '37': [{'node': 'PP', 'head': 'economy-13'}], '38': [{'node': 'IN', 'head': 'of-12'}], '39': [{'node': 'of-12', 'head': 'of-12'}], '40': [{'node': 'NP', 'head': 'economy-13'}], '41': [{'node': 'NN', 'head': 'economy-13'}], '42': [{'node': 'economy-13', 'head': 'economy-13'}], '43': [{'node': '.', 'head': '.-14'}], '44': [{'node': '.-14', 'head': '.-14\\n'}]})}\n"
     ]
    }
   ],
   "source": [
    "syn_dir = \"preprocessing/src/main/resources/LCA_info\"\n",
    "lex_dir = \"preprocessing/src/main/resources/lexico_syntactic\"\n",
    "\n",
    "for file in sorted(os.listdir(syn_dir)):\n",
    "    syn_file = f\"{syn_dir}/{file}\"\n",
    "    lex_file = f\"{lex_dir}/{file}\"\n",
    "    get_lca(syn_file)\n",
    "    get_lex(lex_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
