{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the syntactic and lexico-syntactic features for argument component identification from the output files in the *src/main/resources/syntactic folder* within the **preprocessing** Java project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict \n",
    "\n",
    "def get_lca(syn_file): \n",
    "    ''' \n",
    "    Note that c_token means the constituent type of the current token.\n",
    "    Likewise for the preceding and following tokens, i.e., c_preceding and c_following. \n",
    "    '''\n",
    "    info = defaultdict(list)\n",
    "    with open(syn_file,\"r\") as f: \n",
    "        for line in f.readlines(): \n",
    "            line = line.split(\"\\t\")\n",
    "            sentIdx = line[0]\n",
    "            label = line[2]\n",
    "            position = label.split(\"-\")[1]\n",
    "            if sentIdx not in info: \n",
    "                info[sentIdx] = []\n",
    "            token_info = {\n",
    "                \"token\": line[1], \"position\": int(position), \"c_token\": line[3],\n",
    "                \"preceding\": line[4], \"c_preceding\": line[5], \"lcaPath_preceding\": line[6],\n",
    "                \"following\": line[7], \"c_following\": line[8], \"lcaPath_following\": line[9].strip(\"\\n\")\n",
    "            }\n",
    "            info[sentIdx].append(token_info)\n",
    "            break\n",
    "    print(dict(info))\n",
    "\n",
    "def get_lex(lex_file): \n",
    "    ''' \n",
    "    Info for sentences are divided by newlines. \n",
    "    Formatting of info for each sentence: \n",
    "    1. For each token, <token_label>\\t<label of uppermost node>\n",
    "    2. (if applicable) <token_label>\\t\"child_of_uppermost\"\\t<label of child> \n",
    "    3. (if applicable)  <token_label>\\t\"right_sibling_of_child\"\\t<right_sibling_label>\\t<its lexical head> \n",
    "    4. List with each entry being <node_label>\\t<node_index>\n",
    "    5. HashMap mapping <node_index>=<lexical_head>\n",
    "    '''\n",
    "    token_info = defaultdict(dict)\n",
    "    node_info = {}\n",
    "    sentIdx = 0\n",
    "    node_indices = {}\n",
    "    lexical_heads = defaultdict(list)\n",
    "    with open(lex_file,\"r\") as f: \n",
    "        for line in f.readlines(): \n",
    "            if line == \"\\n\": # reached a new sentence \n",
    "                node_info[sentIdx] = dict(lexical_heads) \n",
    "                print(dict(token_info))\n",
    "                print(dict(node_info))\n",
    "                sentIdx += 1 \n",
    "                node_indices = defaultdict(list)\n",
    "                lexical_heads = defaultdict(list)\n",
    "                if sentIdx == 1: break # for sample printing purposes \n",
    "                continue\n",
    "            if line[0] == \"[\":\n",
    "                # reached info of type 4 \n",
    "                line = line.replace(\"]\",\"\").replace(\"[\",\"\").split(\", \")\n",
    "                for entry in line: \n",
    "                    entry = entry.strip(\"\\n\").split(\"\\t\")\n",
    "                    node_indices[entry[1]] = entry[0]\n",
    "            elif line[0] == \"{\": \n",
    "                # reached info of type 5 \n",
    "                line = line.replace(\"{\",\"\").replace(\"}\",\"\").split(\", \")\n",
    "                for entry in line: \n",
    "                    entry = entry.split(\"=\")\n",
    "                    lexical_heads[entry[0]].append({\"node\": node_indices[entry[0]], \n",
    "                                               \"head\": entry[1] })\n",
    "            else: \n",
    "                line = line.strip(\"\\n\").split(\"\\t\")\n",
    "                token = line[0].split(\"-\")[0]\n",
    "                position = int(line[0].split(\"-\")[1])\n",
    "                if len(line) == 2: \n",
    "                    uppermost = line[1]\n",
    "                    token_dict = { \"token\": token, \"uppermost\": uppermost}\n",
    "                    token_info[sentIdx][position] = token_dict\n",
    "                else: # intermediary info \n",
    "                    type = line[1]\n",
    "                    label = line[2]\n",
    "                    token_info[sentIdx][position][type] = label\n",
    "                    if type == \"right_sibling_of_child\": \n",
    "                        token_info[sentIdx][position][\"right_sibling_type\"] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [{'token': 'It', 'position': 1, 'c_token': 'PRP', 'preceding': '', 'c_preceding': '', 'lcaPath_preceding': '-1.000000', 'following': 'is', 'c_following': 'VBZ', 'lcaPath_following': '0.250000'}]}\n",
      "{0: {1: {'token': 'It', 'uppermost': 'NP', 'child_of_uppermost': 'PRP'}, 2: {'token': 'is', 'uppermost': 'VBZ'}, 3: {'token': 'always', 'uppermost': 'ADVP', 'child_of_uppermost': 'RB'}, 4: {'token': 'said', 'uppermost': 'ROOT', 'child_of_uppermost': 'S'}, 5: {'token': 'that', 'uppermost': 'IN'}, 6: {'token': 'competition', 'uppermost': 'NP', 'child_of_uppermost': 'NN'}, 7: {'token': 'can', 'uppermost': 'MD'}, 8: {'token': 'effectively', 'uppermost': 'ADVP', 'child_of_uppermost': 'RB'}, 9: {'token': 'promote', 'uppermost': 'SBAR', 'child_of_uppermost': 'S'}, 10: {'token': 'the', 'uppermost': 'DT'}, 11: {'token': 'development', 'uppermost': 'NP', 'child_of_uppermost': 'NP', 'right_sibling_of_child ': 'PP'}, 12: {'token': 'of', 'uppermost': 'IN'}, 13: {'token': 'economy', 'uppermost': 'PP', 'child_of_uppermost': 'NP'}, 14: {'token': '.', 'uppermost': '.'}}}\n",
      "{0: {'1': [{'node': 'ROOT', 'head': 'said-4'}], '2': [{'node': 'S', 'head': 'said-4'}], '3': [{'node': 'NP', 'head': 'It-1'}], '4': [{'node': 'PRP', 'head': 'It-1'}], '5': [{'node': 'It-1', 'head': 'It-1'}], '6': [{'node': 'VP', 'head': 'said-4'}], '7': [{'node': 'VBZ', 'head': 'is-2'}], '8': [{'node': 'is-2', 'head': 'is-2'}], '9': [{'node': 'ADVP', 'head': 'always-3'}], '10': [{'node': 'RB', 'head': 'always-3'}], '11': [{'node': 'always-3', 'head': 'always-3'}], '12': [{'node': 'VP', 'head': 'said-4'}], '13': [{'node': 'VBN', 'head': 'said-4'}], '14': [{'node': 'said-4', 'head': 'said-4'}], '15': [{'node': 'SBAR', 'head': 'promote-9'}], '16': [{'node': 'IN', 'head': 'that-5'}], '17': [{'node': 'that-5', 'head': 'that-5'}], '18': [{'node': 'S', 'head': 'promote-9'}], '19': [{'node': 'NP', 'head': 'competition-6'}], '20': [{'node': 'NN', 'head': 'competition-6'}], '21': [{'node': 'competition-6', 'head': 'competition-6'}], '22': [{'node': 'VP', 'head': 'promote-9'}], '23': [{'node': 'MD', 'head': 'can-7'}], '24': [{'node': 'can-7', 'head': 'can-7'}], '25': [{'node': 'ADVP', 'head': 'effectively-8'}], '26': [{'node': 'RB', 'head': 'effectively-8'}], '27': [{'node': 'effectively-8', 'head': 'effectively-8'}], '28': [{'node': 'VP', 'head': 'promote-9'}], '29': [{'node': 'VB', 'head': 'promote-9'}], '30': [{'node': 'promote-9', 'head': 'promote-9'}], '31': [{'node': 'NP', 'head': 'development-11'}], '32': [{'node': 'NP', 'head': 'development-11'}], '33': [{'node': 'DT', 'head': 'the-10'}], '34': [{'node': 'the-10', 'head': 'the-10'}], '35': [{'node': 'NN', 'head': 'development-11'}], '36': [{'node': 'development-11', 'head': 'development-11'}], '37': [{'node': 'PP', 'head': 'economy-13'}], '38': [{'node': 'IN', 'head': 'of-12'}], '39': [{'node': 'of-12', 'head': 'of-12'}], '40': [{'node': 'NP', 'head': 'economy-13'}], '41': [{'node': 'NN', 'head': 'economy-13'}], '42': [{'node': 'economy-13', 'head': 'economy-13'}], '43': [{'node': '.', 'head': '.-14'}], '44': [{'node': '.-14', 'head': '.-14\\n'}]}}\n"
     ]
    }
   ],
   "source": [
    "syn_dir = \"preprocessing/src/main/resources/syntactic/\"\n",
    "lca_dir = syn_dir + \"LCA_info\"\n",
    "lex_dir = syn_dir + \"lexico_syntactic\"\n",
    "verb_dir = syn_dir + \"main_verbs\"\n",
    "\n",
    "for file in sorted(os.listdir(lca_dir)):\n",
    "    syn_file = f\"{lca_dir}/{file}\"\n",
    "    lex_file = f\"{lex_dir}/{file}\"\n",
    "    get_lca(syn_file)\n",
    "    get_lex(lex_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
